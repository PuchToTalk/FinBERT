{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPn25Nk22EmFYIN3886+H63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PuchToTalk/FinBERT/blob/fine-tuning/Fine_Tuning_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "43PW_bI7MH6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8300d258-8ee5-4136-9db9-fd26ec89bcb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GwWlqOA3LR6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ef6712-8892-4f2a-b144-25606280bd3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.7307\n",
            "Epoch 2/50, Loss: 0.6575\n",
            "Epoch 3/50, Loss: 0.7248\n",
            "Epoch 4/50, Loss: 0.6236\n",
            "Epoch 5/50, Loss: 0.6837\n",
            "Epoch 6/50, Loss: 0.6357\n",
            "Epoch 7/50, Loss: 0.5782\n",
            "Epoch 8/50, Loss: 0.6480\n",
            "Epoch 9/50, Loss: 0.5875\n",
            "Epoch 10/50, Loss: 0.5165\n",
            "Epoch 11/50, Loss: 0.4752\n",
            "Epoch 12/50, Loss: 0.4949\n",
            "Epoch 13/50, Loss: 0.4396\n",
            "Epoch 14/50, Loss: 0.4192\n",
            "Epoch 15/50, Loss: 0.3974\n",
            "Epoch 16/50, Loss: 0.3918\n",
            "Epoch 17/50, Loss: 0.3716\n",
            "Epoch 18/50, Loss: 0.3469\n",
            "Epoch 19/50, Loss: 0.3157\n",
            "Epoch 20/50, Loss: 0.2757\n",
            "Epoch 21/50, Loss: 0.2462\n",
            "Epoch 22/50, Loss: 0.2451\n",
            "Epoch 23/50, Loss: 0.2750\n",
            "Epoch 24/50, Loss: 0.2466\n",
            "Epoch 25/50, Loss: 0.2245\n",
            "Epoch 26/50, Loss: 0.2063\n",
            "Epoch 27/50, Loss: 0.2097\n",
            "Epoch 28/50, Loss: 0.1817\n",
            "Epoch 29/50, Loss: 0.1755\n",
            "Epoch 30/50, Loss: 0.1847\n",
            "Epoch 31/50, Loss: 0.1468\n",
            "Epoch 32/50, Loss: 0.1522\n",
            "Epoch 33/50, Loss: 0.1538\n",
            "Epoch 34/50, Loss: 0.1467\n",
            "Epoch 35/50, Loss: 0.1134\n",
            "Epoch 36/50, Loss: 0.1115\n",
            "Epoch 37/50, Loss: 0.1140\n",
            "Epoch 38/50, Loss: 0.1300\n",
            "Epoch 39/50, Loss: 0.1240\n",
            "Epoch 40/50, Loss: 0.0934\n",
            "Epoch 41/50, Loss: 0.0829\n",
            "Epoch 42/50, Loss: 0.0870\n",
            "Epoch 43/50, Loss: 0.0759\n",
            "Epoch 44/50, Loss: 0.0715\n",
            "Epoch 45/50, Loss: 0.0750\n",
            "Epoch 46/50, Loss: 0.0693\n",
            "Epoch 47/50, Loss: 0.0656\n",
            "Epoch 48/50, Loss: 0.0696\n",
            "Epoch 49/50, Loss: 0.0518\n",
            "Epoch 50/50, Loss: 0.0579\n",
            "Validation Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "\n",
        "# Define your own dataset for text classification\n",
        "# In this example, let's assume you have training_data and validation_data\n",
        "# training_data should be a list of (text, label) pairs\n",
        "# validation_data should be a list of (text, label) pairs\n",
        "\n",
        "# Define the training data as a list of (text, label) pairs\n",
        "training_data = [\n",
        "    (\"I love this person.\", 1),             # Positive sentiment (label 1)\n",
        "    (\"I hate reading.\", 0),                # Negative sentiment (label 0)\n",
        "    (\"The weather is beautiful today.\", 1), # Positive sentiment (label 1)\n",
        "    (\"I'm feeling great.\", 1),             # Positive sentiment (label 1)\n",
        "    (\"I don't like this movie.\", 0),        # Negative sentiment (label 0)\n",
        "    (\"I enjoy spending time with my family.\", 1),  # Positive sentiment (label 1)\n",
        "    (\"Studying can be boring.\", 0),              # Negative sentiment (label 0)\n",
        "    (\"The food at that restaurant was delicious.\", 1),  # Positive sentiment (label 1)\n",
        "    (\"I had a terrible day at work.\", 0),      # Negative sentiment (label 0)\n",
        "    (\"I am a horrible person\", 0)      # Negative sentiment (label 0)\n",
        "    # Add more training data examples here...\n",
        "]\n",
        "\n",
        "# Define the validation data as a list of (text, label) pairs\n",
        "validation_data = [\n",
        "    (\"I love this dog.\", 1),               # Positive sentiment (label 1)\n",
        "    (\"I hate doing my homework.\", 0),      # Negative sentiment (label 0)\n",
        "    (\"The sunset was breathtaking.\", 1),   # Positive sentiment (label 1)\n",
        "    (\"I'm not in a good mood.\", 0),        # Negative sentiment (label 0)\n",
        "    (\"This book is amazing!\", 1),          # Positive sentiment (label 1)\n",
        "    (\"I enjoyed the concert last night.\", 1),  # Positive sentiment (label 1)\n",
        "    (\"I can't stand the traffic in this city.\", 0),  # Negative sentiment (label 0)\n",
        "    (\"The vacation was relaxing and fun.\", 1),  # Positive sentiment (label 1)\n",
        "    (\"I'm frustrated with my computer.\", 0),    # Negative sentiment (label 0)\n",
        "    (\"I am a awful person\", 0)      # Negative sentiment (label 0)\n",
        "    # Add more validation data examples here...\n",
        "]\n",
        "\n",
        "\n",
        "# Define the BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can choose other pre-trained models\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Change num_labels to match your classification task\n",
        "\n",
        "# Tokenize and preprocess the data\n",
        "def preprocess_data(data):\n",
        "    inputs = [tokenizer.encode(text, add_special_tokens=True, max_length=128, pad_to_max_length=True) for text, _ in data]\n",
        "    labels = [label for _, label in data]\n",
        "    inputs = torch.tensor(inputs)\n",
        "    labels = torch.tensor(labels)\n",
        "    return inputs, labels\n",
        "\n",
        "train_inputs, train_labels = preprocess_data(training_data)\n",
        "val_inputs, val_labels = preprocess_data(validation_data)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = TensorDataset(val_inputs, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define training parameters\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 50  # You can adjust the number of epochs\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Fine-Tune the BERT model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)  # Move the model to the specified device\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)  # Move inputs to the same device as the model\n",
        "        labels = labels.to(device)  # Move labels to the same device as the model\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        labels = labels.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_bert\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows the training and validation progress for a BERT-based text classification model. Here's how to interpret the results:\n",
        "\n",
        "**Epoch 1/3, Loss: 0.7588:**\n",
        "\n",
        "\n",
        "> This indicates that the model has completed the first training epoch.\n",
        "The loss of approximately 0.7588 is the average loss calculated over all batches in the training data during this epoch.\n",
        "Loss measures how well the model is performing; a lower loss is better. It represents the error between the model's predictions and the actual labels.\n",
        "\n",
        "\n",
        "\n",
        "**Epoch 2/3, Loss: 0.7083:**\n",
        "\n",
        "> This shows the results after the second training epoch.\n",
        "The loss has decreased to approximately 0.7083, which is expected during training as the model learns to make better predictions.\n",
        "\n",
        "**Epoch 3/3, Loss: 0.6144:**\n",
        "\n",
        "> This is the result after the third and final training epoch.\n",
        "The loss has decreased further to approximately 0.6144, indicating that the model continues to improve.\n",
        "Validation Accuracy: 0.5000:\n",
        "\n",
        "The validation accuracy of 0.5000 means that, when evaluating the model on the validation dataset, it correctly predicted the labels for 50% of the examples.\n",
        "Validation accuracy is a common metric used to evaluate classification models. In this case, it indicates that the model is performing at a random or chance level, as it's correctly classifying roughly half of the examples.\n",
        "\n",
        "\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "The decreasing training loss across epochs is a positive sign, suggesting that the model is learning and improving its predictions on the training data.\n",
        "However, the low validation accuracy of 0.5000 indicates that the model's performance on unseen data (validation data) is no better than random guessing. This suggests that the model might be underfitting or that the data and model architecture may require further tuning.\n"
      ],
      "metadata": {
        "id": "nA3u8HLFGlre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input data\n",
        "test_data = [\n",
        "    (\"I am happy.\", 1),          # Positive sentiment\n",
        "    (\"I am sad.\", 0),            # Negative sentiment\n",
        "    (\"The movie was great.\", 1), # Positive sentiment\n",
        "    (\"This person was awful.\", 0),  # Negative sentiment\n",
        "    # Add more test data examples here...\n",
        "]\n",
        "\n",
        "test_inputs, test_labels = preprocess_data(test_data)\n",
        "\n",
        "# Create a data loader for testing\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)  # Move inputs to the same device as the model\n",
        "        labels = labels.to(device)  # Move labels to the same device as the model\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7wVFyp1NkwB",
        "outputId": "2991aa3b-8d03-4a5f-d100-e28db99f353c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5V6ZKTQMsaS",
        "outputId": "08a948f3-9980-4ded-f712-561a58fb6266"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0, 1, 0], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the fine-tuned model"
      ],
      "metadata": {
        "id": "DQ09GA7hNEE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"fine_tuned_bert\"  # Path to the directory where the model is saved\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Izk7-9zsGwA6",
        "outputId": "2e913d1a-df06-424b-86df-8521de3313d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4eb70a0c0451>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fine_tuned_bert\"\u001b[0m  \u001b[0;31m# Path to the directory where the model is saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1839\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'fine_tuned_bert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'fine_tuned_bert' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
          ]
        }
      ]
    }
  ]
}