{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT Example Notebook\n",
    "\n",
    "This notebooks shows how to train and use the FinBERT pre-trained language model for financial sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:04.902740Z",
     "start_time": "2020-03-23T15:55:04.876252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement finbert (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for finbert\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulc/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement finbert (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for finbert\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sg/d_52nb_57s30gj20d0hs23bm0000gn/T/ipykernel_4764/4074578675.py:22: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m project_dir \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m     22\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwget\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "!pip install finbert\n",
    "from textblob import TextBlob\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "!pip install finbert\n",
    "\n",
    "from finbert.finbert import *\n",
    "import finbert.utils as tools\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "project_dir = Path.cwd().parent\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "\n",
    "import wget\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from plottify import autosize\n",
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:05.711210Z",
     "start_time": "2020-03-23T15:55:05.693609Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting path variables:\n",
    "1. `lm_path`: the path for the pre-trained language model (If vanilla Bert is used then no need to set this one).\n",
    "2. `cl_path`: the path where the classification model is saved.\n",
    "3. `cl_data_path`: the path of the directory that contains the data files of `train.csv`, `validation.csv`, `test.csv`.\n",
    "---\n",
    "\n",
    "In the initialization of `bertmodel`, we can either use the original pre-trained weights from Google by giving `bm = 'bert-base-uncased`, or our further pre-trained language model by `bm = lm_path`\n",
    "\n",
    "\n",
    "---\n",
    "All of the configurations with the model is controlled with the `config` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:07.405597Z",
     "start_time": "2020-03-23T15:55:07.386378Z"
    }
   },
   "outputs": [],
   "source": [
    "#lm_path = project_dir/'models'/'language_model'/'finbertTRC2'\n",
    "#/'pytorch_model.bin'\n",
    "lm_path = project_dir/'models'/'sentiment'\n",
    "cl_path = project_dir/'models'/'classifier_model'/'finbert-sentiment'\n",
    "cl_data_path = project_dir/'data'/'sentiment_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Configuring training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the explanations of the training parameters in the class docsctrings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:12.378583Z",
     "start_time": "2020-03-23T15:55:09.196746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean the cl_path\n",
    "try:\n",
    "    shutil.rmtree(cl_path) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "bertmodel = AutoModelForSequenceClassification.from_pretrained(lm_path,cache_dir=None, num_labels=3)\n",
    "\n",
    "\n",
    "config = Config(   data_dir=cl_data_path,\n",
    "                   bert_model=bertmodel,\n",
    "                   num_train_epochs=4,\n",
    "                   model_dir=cl_path,\n",
    "                   max_seq_length = 48,\n",
    "                   train_batch_size = 32,\n",
    "                   learning_rate = 2e-5,\n",
    "                   output_mode='classification',\n",
    "                   warm_up_proportion=0.2,\n",
    "                   local_rank=-1,\n",
    "                   discriminate=True,\n",
    "                   gradual_unfreeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = FinBert(config)\n",
    "finbert.base_model = 'bert-base-uncased'\n",
    "finbert.config.discriminate=True\n",
    "finbert.config.gradual_unfreeze=True\n",
    "\n",
    "finbert.prepare_model(label_list=['positive','negative','neutral'])\n",
    "train_data = finbert.get_data('train')\n",
    "train_dataloader = finbert.get_loader(train_data, 'train')\n",
    "\n",
    "model = finbert.create_the_model()\n",
    "\n",
    "\n",
    "# Get the training examples\n",
    "train_data = finbert.get_data('train')\n",
    "train_dataloader = finbert.get_loader(train_data, 'train')\n",
    "test_data = finbert.get_data('test')\n",
    "test_dataloader = finbert.get_loader(test_data, 'train')\n",
    "\n",
    "\n",
    "\n",
    "inputs=torch.tensor([]).to(\"cuda\")\n",
    "mask=torch.tensor([]).to(\"cuda\")\n",
    "type_ids=torch.tensor([]).to(\"cuda\")\n",
    "labels=torch.tensor([]).to(\"cuda\")\n",
    "\n",
    "for step, batch in enumerate(tqdm(train_dataloader, desc='Iteration')):\n",
    "    batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "    input_ids_temp, attention_mask, token_type_ids, label_ids_temp, agree_ids = batch\n",
    "    #label_ids_temp[label_ids_temp==1]=0\n",
    "    #label_ids_temp[label_ids_temp==2]=1\n",
    "\n",
    "\n",
    "    labels=torch.cat((labels,label_ids_temp))\n",
    "    inputs=torch.cat((inputs,input_ids_temp),axis=0)\n",
    "    mask=torch.cat((mask,attention_mask),axis=0)\n",
    "    type_ids=torch.cat((type_ids,token_type_ids),axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "inputs_test=torch.tensor([]).to(\"cuda\")\n",
    "mask_test=torch.tensor([]).to(\"cuda\")\n",
    "type_ids_test=torch.tensor([]).to(\"cuda\")\n",
    "\n",
    "labels_test=torch.tensor([]).to(\"cuda\")\n",
    "\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc='Iteration')):\n",
    "    batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "    input_ids_temp, attention_mask, token_type_ids, label_ids_temp, agree_ids = batch\n",
    "    #label_ids_temp[label_ids_temp==1]=0\n",
    "    #label_ids_temp[label_ids_temp==2]=1\n",
    "\n",
    "\n",
    "    labels_test=torch.cat((labels_test,label_ids_temp))\n",
    "    inputs_test=torch.cat((inputs_test,input_ids_temp),axis=0)\n",
    "    mask_test=torch.cat((mask_test,attention_mask),axis=0)\n",
    "    type_ids_test=torch.cat((type_ids_test,token_type_ids),axis=0)\n",
    "# print(labels.shape,inputs.shape)\n",
    "# print(labels,inputs)\n",
    "maxlen=torch.max(inputs).to(\"cpu\").int().numpy()\n",
    "print(maxlen)\n",
    "print(inputs.int())\n",
    "inputs=inputs.int()\n",
    "labels=labels.long()\n",
    "inputs_test=inputs_test.int()\n",
    "labels_test=labels_test.long()\n",
    "\n",
    "data=torch.cat((inputs,labels.reshape(-1,1)),dim=1).to(\"cpu\")\n",
    "data_test=torch.cat((inputs_test,labels_test.reshape(-1,1)),dim=1).to(\"cpu\")\n",
    "\n",
    "adds={}\n",
    "adds[\"mask\"]=mask.T\n",
    "adds[\"mask_test\"]=mask_test.T\n",
    "adds[\"type_ids\"]=type_ids.T\n",
    "adds[\"type_ids_test\"]=type_ids_test.T\n",
    "\n",
    "data_l=list(zip(data,mask,type_ids))\n",
    "\n",
    "print(data.shape,data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`finbert` is our main class that encapsulates all the functionality. The list of class labels should be given in the prepare_model method call with label_list parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "bertmodel = AutoModelForSequenceClassification.from_pretrained(lm_path,cache_dir=None, num_labels=3)\n",
    "config = Config(   data_dir=cl_data_path,\n",
    "                   bert_model=bertmodel,\n",
    "                   num_train_epochs=4,\n",
    "                   model_dir=cl_path,\n",
    "                   max_seq_length = 48,\n",
    "                   train_batch_size = 32,\n",
    "                   learning_rate = 2e-5,\n",
    "                   output_mode='classification',\n",
    "                   warm_up_proportion=0.2,\n",
    "                   local_rank=-1,\n",
    "                   discriminate=True,\n",
    "                   gradual_unfreeze=True)\n",
    "\n",
    "finbert = FinBert(config)\n",
    "finbert.base_model = 'bert-base-uncased'\n",
    "finbert.config.discriminate=True\n",
    "finbert.config.gradual_unfreeze=False\n",
    "finbert.prepare_model(label_list=['positive','negative','neutral'])\n",
    "\n",
    "finbert.prepare_model(label_list=['positive','negative','neutral'])\n",
    "train_data = finbert.get_data('train')\n",
    "train_dataloader = finbert.get_loader(train_data, 'train')\n",
    "\n",
    "fb=finbert.create_the_model()\n",
    "for param in fb.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(12):\n",
    "    for param in fb.bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "for param in fb.bert.pooler.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in fb.dropout.parameters():\n",
    "    param.requires_grad = False\n",
    "# fb.dropout=Identity()\n",
    "fb.classifier=Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = data.T, data_test.T\n",
    "\n",
    "print(train_data.shape)\n",
    "mask=adds[\"mask\"]\n",
    "ids=adds[\"type_ids\"]\n",
    "embeds=torch.zeros((train_data.shape[1],768))\n",
    "labels=torch.zeros((train_data.shape[1],))\n",
    "for dt, msk, ids in [(train_data, adds[\"mask\"], adds[\"type_ids\"])]:\n",
    "    model.train(False)\n",
    "    dl = torch.split(dt, 32, dim=1)\n",
    "    ml = torch.split(msk, 32, dim=1)\n",
    "    il = torch.split(ids, 32, dim=1)\n",
    "    for i_dl, input in enumerate(dl):\n",
    "        input=input.to(\"cuda\")\n",
    "        with torch.set_grad_enabled(False):\n",
    "            labels[i_dl*32:(i_dl+1)*32] = input[-1]\n",
    "            embeds[i_dl*32:(i_dl+1)*32,:] = fb(input[:-1].T.int(),ml[i_dl].T.int(),il[i_dl].T.int())[0]\n",
    "\n",
    "mask_test=adds[\"mask_test\"].T\n",
    "ids_test=adds[\"type_ids_test\"].T         \n",
    "embeds_test=torch.zeros((valid_data.shape[1],768))\n",
    "labels_test=torch.zeros((valid_data.shape[1],))\n",
    "for dt, msk, ids in [(valid_data, adds[\"mask_test\"], adds[\"type_ids_test\"])]:\n",
    "    model.train(False)\n",
    "    dl = torch.split(dt, 32, dim=1)\n",
    "    ml = torch.split(msk, 32, dim=1)\n",
    "    il = torch.split(ids, 32, dim=1)\n",
    "    for i_dl, input in enumerate(dl):\n",
    "        input=input.to(\"cuda\")\n",
    "        with torch.set_grad_enabled(False):\n",
    "            labels_test[i_dl*32:(i_dl+1)*32] = input[-1]\n",
    "            embeds_test[i_dl*32:(i_dl+1)*32,:] = fb(input[:-1].T.int(),ml[i_dl].T.int(),il[i_dl].T.int())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def one_hot(labels, num_classes=10):\n",
    "    if torch.is_tensor(labels):\n",
    "        y = torch.eye(num_classes) \n",
    "        return y[labels.long()] \n",
    "    else:\n",
    "        y = np.eye(num_classes) \n",
    "        return y[labels.astype(\"long\")] \n",
    "def drelu(x):\n",
    "    return x>=0\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def samp(Xcvx, Xcvx_test, act=\"relu\", depth=2):\n",
    "    d=Xcvx.shape[1]\n",
    "    if depth==2:\n",
    "        U=np.random.randn(d,num_neurons)\n",
    "        preact=Xcvx@U\n",
    "        b=np.random.uniform(np.min(preact),np.max(preact),num_neurons)\n",
    "        if act == \"relu\":\n",
    "            #dmat=drelu(Xcvx@U)\n",
    "            dmat=drelu(preact-b.reshape(1,-1))\n",
    "        else:\n",
    "            dmat=np.sign(preact-b.reshape(1,-1))\n",
    "        ### eliminate repeating arrangements\n",
    "        dmat, ind=(np.unique(dmat,axis=1, return_index=True))\n",
    "        U=U[:,ind]\n",
    "        b=b[ind]\n",
    "        if act == \"relu\":\n",
    "            dmat_test=drelu(Xcvx_test@U-b.reshape(1,-1))\n",
    "        else:\n",
    "            dmat_test=np.sign(Xcvx_test@U-b.reshape(1,-1))\n",
    "    else:\n",
    "        mvec=[d]*(depth-1)\n",
    "        mvec.append(num_neurons)\n",
    "        A=Xcvx.copy()\n",
    "        Atest=Xcvx_test.copy()\n",
    "        for l in range(depth-1):\n",
    "            U=np.random.randn(mvec[l],mvec[l+1])\n",
    "            preact=A@U\n",
    "            #print(np.max(preact),np.min(preact))\n",
    "            b=np.zeros((mvec[l+1]))\n",
    "            for i_col in range(mvec[l+1]):\n",
    "                b[i_col]=np.random.uniform(np.min(preact[:,i_col]),np.max(preact[:,i_col]))\n",
    "                # print(np.min(preact[:,i_col]),np.max(preact[:,i_col]),b[i_col])\n",
    "                # plt.hist(preact[:,i_col], bins=preact[:,i_col].shape[0])\n",
    "                # plt.axvline(b[i_col], color=\"red\")\n",
    "                # plt.show()\n",
    "            #b=np.random.uniform(np.min(preact),np.max(preact),mvec[l+1])\n",
    "            # mean1=np.mean((A@U), axis=0, keepdims=True)\n",
    "            # std1=np.std((A@U), axis=0, keepdims=True)\n",
    "            hidden=preact-b.reshape(1,-1)#(A@U-mean1)/std1\n",
    "            hidden_test=Atest@U-b.reshape(1,-1)#((Atest@U)-mean1)/std1\n",
    "            hidden[np.isnan(hidden)]=0\n",
    "            hidden_test[np.isnan(hidden_test)]=0\n",
    "            if l<depth-2:\n",
    "                A=relu(hidden) #+ A\n",
    "                Atest=relu(hidden_test) #+ Atest\n",
    "                amax=np.max(A)\n",
    "                A/=amax\n",
    "                Atest/=amax\n",
    "                A-=0.5\n",
    "                Atest-=0.5\n",
    "                \n",
    "                # mean1=np.mean(A, axis=0, keepdims=True)\n",
    "                # std1=np.std(A, axis=0, keepdims=True)\n",
    "                # A=(A-mean1)/std1\n",
    "                # Atest=(Atest-mean1)/std1               \n",
    "            else:\n",
    "                print(\"Activation used for sampling: \", act)\n",
    "                if act == \"relu\":\n",
    "                    dmat=drelu(hidden)\n",
    "                else:\n",
    "                    dmat=np.sign(hidden)\n",
    "                ### eliminate repeating arrangements\n",
    "                dmat, ind=(np.unique(dmat,axis=1, return_index=True))\n",
    "                m=dmat.shape[1]\n",
    "                if act == \"relu\":\n",
    "                    dmat_test=drelu(hidden_test[:,ind])\n",
    "                else:\n",
    "                    dmat_test=np.sign(hidden_test[:,ind])\n",
    "            #print(hidden)\n",
    "    return dmat, dmat_test\n",
    "\n",
    "                   \n",
    "def CVX_MLP(Xcvx, y, Xcvx_test, ytest, mode=\"relaxed\", act=\"relu\", num_neurons=100, depth=2, beta=1e-4, num_classes=3, seed=0):\n",
    "    random.seed(a=seed)\n",
    "    np.random.seed(seed=seed)\n",
    "    mode=\"relaxed\"\n",
    "    dmat, dmat_test = samp(Xcvx[:,:-1], Xcvx_test[:,:-1], act=act, depth=depth)\n",
    "    m=dmat.shape[1]        \n",
    "    print(\"Number of unique arrangements: \", m)\n",
    "\n",
    "    Y=one_hot(y, num_classes)\n",
    "\n",
    "    if mode==\"exact\":\n",
    "        # Optimal CVX\n",
    "        m=dmat.shape[1]\n",
    "        Uopt1={}\n",
    "        Uopt2={}\n",
    "        for j in range(num_classes):\n",
    "            Uopt1[j]=cp.Variable((d,m))\n",
    "            Uopt2[j]=cp.Variable((d,m))\n",
    "\n",
    "        ## Below we use hinge loss as a performance metric for binary classification\n",
    "        Yopt1={}\n",
    "        Yopt2={}\n",
    "        for j in range(num_classes):\n",
    "            Yopt1[j]=cp.Parameter((n,))\n",
    "            Yopt2[j]=cp.Parameter((n,))\n",
    "        reg=cp.Parameter((1))\n",
    "        reg=0\n",
    "        cost=0\n",
    "        for j in range(num_classes):\n",
    "            Yopt1[j]=cp.sum(cp.multiply(dmat,(Xcvx*Uopt1[j])),axis=1)\n",
    "            Yopt2[j]=cp.sum(cp.multiply(dmat,(Xcvx*Uopt2[j])),axis=1)\n",
    "            reg=(cp.mixed_norm(Uopt1[j].T,2,1)+cp.mixed_norm(Uopt2[j].T,2,1))\n",
    "            cost+=cp.sum_squares(Y[:,j]-(Yopt1[j]-Yopt2[j]))/n+beta*reg\n",
    "        constraints=[]\n",
    "        # for j in range(num_classes):\n",
    "        #     constraints+=[cp.multiply((2*dmat-np.ones((n,m1))),(X*Uopt1[j]))>=0]\n",
    "        #     constraints+=[cp.multiply((2*dmat-np.ones((n,m1))),(X*Uopt2[j]))>=0]\n",
    "        prob=cp.Problem(cp.Minimize(cost),constraints)\n",
    "        prob.solve(solver=cp.MOSEK,warm_start=True, verbose=True)\n",
    "\n",
    "        cvx_opt=prob.value\n",
    "        print(\"Convex program objective value (eq (8)): \",cvx_opt)\n",
    "    else:\n",
    "        # Optimal CVX\n",
    "        m1=dmat.shape[1]\n",
    "        Uopt1={}\n",
    "        for j in range(num_classes):\n",
    "            Uopt1[j]=cp.Variable((d,m))\n",
    "\n",
    "        ## Below we use hinge loss as a performance metric for binary classification\n",
    "        Yopt1={}\n",
    "        for j in range(num_classes):\n",
    "            Yopt1[j]=cp.Parameter((n,))\n",
    "        reg=cp.Parameter((1))\n",
    "        reg=0\n",
    "        cost=0\n",
    "        for j in range(num_classes):\n",
    "            Yopt1[j]=cp.sum(cp.multiply(dmat,(Xcvx*Uopt1[j])),axis=1)\n",
    "            reg=(cp.mixed_norm(Uopt1[j].T,2,1))\n",
    "            cost+=cp.sum_squares(Y[:,j]-Yopt1[j])/n+beta*reg\n",
    "        constraints=[]\n",
    "        # for j in range(num_classes):\n",
    "        #     constraints+=[cp.multiply((2*dmat-np.ones((n,m1))),(X*Uopt1[j]))>=0]\n",
    "        #     constraints+=[cp.multiply((2*dmat-np.ones((n,m1))),(X*Uopt2[j]))>=0]\n",
    "        prob=cp.Problem(cp.Minimize(cost),constraints)\n",
    "        prob.solve(solver=cp.MOSEK,warm_start=True, verbose=False)\n",
    "\n",
    "        cvx_opt=prob.value\n",
    "    \n",
    "    if prob.status != \"optimal\":\n",
    "        print(\"Convex: Status convex: \",prob.status)\n",
    "    Uopt={}\n",
    "    Ytest_est=np.zeros((ntest,num_classes))\n",
    "    for j in range(num_classes):\n",
    "        Uopt[j]=Uopt1[j].value\n",
    "        Ytest_est[:,j]=np.sum(dmat_test*(Xcvx_test@Uopt[j]),axis=1)\n",
    "\n",
    "\n",
    "    labels_est=np.argmax(Ytest_est,axis=1)\n",
    "    test_acc=np.sum(ytest==labels_est)/ntest\n",
    "    return test_acc\n",
    "\n",
    "class TMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, d, num_neurons=1,  num_outs=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d, num_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_neurons, num_outs),\n",
    "        )      \n",
    "        self.d=d\n",
    "        self.num_neurons=num_neurons\n",
    "        self.num_outs=num_outs\n",
    "    def forward(self, x):\n",
    "        y_pred=self.mlp(x)\n",
    "        return y_pred\n",
    "\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "def NCVX_MLP(Xcvx, y, Xcvx_test, ytest, train_epochs=100, learning_rate=1e-3, num_neurons=100, \\\n",
    "                                                    beta=1e-4, batch_size=32, num_classes=3, seed=0, device=\"cuda\"):\n",
    "    random.seed(a=seed)\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    " \n",
    "    model = TMLP(Xcvx.shape[1], num_neurons=num_neurons, num_outs=num_classes).to(device)\n",
    "    X=torch.tensor(Xcvx).float()\n",
    "    Xtest=torch.tensor(Xcvx_test).float()\n",
    "    label=torch.tensor(y).float()\n",
    "    labeltest=torch.tensor(ytest).float()\n",
    "    train_data = [(dt, lb) for dt, lb in zip(X,label) ]\n",
    "    test_data = [(dt, lb) for dt, lb in zip(Xtest,labeltest) ]  \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "    budget = np.floor(train_epochs*(X.shape[0]//batch_size))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=budget//4,\n",
    "                                                       verbose=False,\n",
    "                                                       factor=0.5,\n",
    "                                                       eps=1e-12)\n",
    "    loss_vec=[]\n",
    "    for epoch in range(train_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, one_hot(labels, num_classes=num_classes).to(device))\n",
    "   \n",
    "            for param in model.parameters():\n",
    "                loss += beta*torch.norm(param)**2/2             \n",
    "         \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            loss_vec.append(loss.item())\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "\n",
    "    # plt.semilogy(loss_vec,label=model_type)\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct/total\n",
    "\n",
    "def LinearModel(A, y, lamb=0):\n",
    "    n_col = A.shape[1]\n",
    "    return np.linalg.lstsq(A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y),rcond=None)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvec=[ 200, 400, 600, 800, 1000]#, 1200, 1400, 1600, 1800, 2000]#, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
    "nonconvex=[]\n",
    "convex=[]\n",
    "convex_deep=[]\n",
    "linear=[]\n",
    "seeds=[0]#,10,20]\n",
    "for seed in seeds:\n",
    "    print(\"seed: \", seed)\n",
    "    random.seed(a=seed)\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    for nt in nvec:\n",
    "        sind=np.random.choice(embeds.shape[0],nt,replace=False)\n",
    "        Xcvx=embeds[sind,:].cpu().numpy()\n",
    "        Xcvx_test=embeds_test.cpu().numpy()\n",
    "        y=labels[sind].cpu().numpy()\n",
    "        ytest=labels_test.cpu().numpy()\n",
    "\n",
    "        Xcvx=np.concatenate((Xcvx,np.ones((Xcvx.shape[0],1))),axis=1)\n",
    "        Xcvx_test=np.concatenate((Xcvx_test,np.ones((Xcvx_test.shape[0],1))),axis=1)\n",
    "\n",
    "        n,d=Xcvx.shape\n",
    "        ntest=Xcvx_test.shape[0]\n",
    "\n",
    "        print(n,d)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        train_epochs = 100\n",
    "        num_neurons=100\n",
    "        batch_size=32\n",
    "        depth=8\n",
    "        accuracy_list = {}\n",
    "        accuracy_list['convex'] = []\n",
    "        accuracy_list['convex_deep'] = []\n",
    "        accuracy_list['nonconvex'] = []\n",
    "        accuracy_list['linear'] = []\n",
    "        test_arr=[]\n",
    "        hyper_param=[]\n",
    "        models=[ 'convex_deep', 'convex', 'nonconvex', 'linear']\n",
    "        for model_type in models:\n",
    "            print('model_type: ', model_type)\n",
    "            #create a dictionary to store the accuracies for  'convex' and 'nonconvex'\n",
    "            for beta in [1e-3]:\n",
    "                #print('beta: ', beta)\n",
    "                for i_lr, lr in enumerate([1e-1, 1e-2, 1e-3, 1e-4]):\n",
    "                    if model_type == 'convex':\n",
    "                        if i_lr==0:\n",
    "                            test_acc = CVX_MLP(Xcvx, y, Xcvx_test, ytest, mode=\"relaxed\", num_neurons=num_neurons, \\\n",
    "                                               beta=beta, depth=2, num_classes=3, seed=seed)\n",
    "                    elif model_type == 'convex_deep':\n",
    "                        if i_lr==0:\n",
    "                            test_acc = CVX_MLP(Xcvx, y, Xcvx_test, ytest, mode=\"relaxed\", num_neurons=num_neurons, \\\n",
    "                                               beta=beta, depth=depth, num_classes=3, seed=seed)\n",
    "                    elif model_type == 'nonconvex':\n",
    "                        test_acc = NCVX_MLP(Xcvx[:,:-1], y, Xcvx_test[:,:-1], ytest, train_epochs=train_epochs, learning_rate=lr, \\\n",
    "                                            num_neurons=num_neurons, beta=beta, batch_size=batch_size, num_classes=3, seed=seed)\n",
    "                    else:\n",
    "                        if i_lr==0:\n",
    "                            model=LinearModel(Xcvx,one_hot(y,num_classes=3),lamb=beta*Xcvx.shape[0])\n",
    "                            est_test=Xcvx_test@model\n",
    "                            test_acc=np.sum(np.argmax(est_test,axis=1)==ytest)/ytest.shape[0]\n",
    "\n",
    "                    test_arr.append(100 * test_acc)\n",
    "                    hyper_param.append([beta,lr])\n",
    "                    if model_type == 'convex':\n",
    "                        if i_lr==0:\n",
    "                            accuracy_list['convex'].append(test_acc)\n",
    "                    elif model_type == 'convex_deep':\n",
    "                        if i_lr==0:\n",
    "                            accuracy_list['convex_deep'].append(test_acc)\n",
    "                    elif model_type == 'nonconvex':\n",
    "                        accuracy_list['nonconvex'].append(test_acc)\n",
    "                    else:\n",
    "                        accuracy_list['linear'].append(test_acc)\n",
    "                    \n",
    "                    if (model_type in [\"convex\",\"convex_deep\",\"linear\"] and i_lr==0) or model_type==\"nonconvex\":\n",
    "                        print(f'Accuracy for beta={beta}, LR={lr}: {np.round(test_arr[-1],3)}')\n",
    "\n",
    "            #print the largest accuracy for each model type\n",
    "        for model_type in models:\n",
    "            print(model_type+' max accuracy: ', max(accuracy_list[model_type]))\n",
    "            print(accuracy_list[model_type])\n",
    "            if model_type == 'convex':\n",
    "                convex.append(max(accuracy_list[model_type]))\n",
    "            elif model_type == 'convex_deep':\n",
    "                convex_deep.append(max(accuracy_list[model_type]))\n",
    "            elif model_type == 'nonconvex':\n",
    "                nonconvex.append(max(accuracy_list[model_type]))\n",
    "            else:\n",
    "                linear.append(max(accuracy_list[model_type]))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convexr=np.array(convex).reshape(len(seeds),-1)\n",
    "convex_deepr=np.array(convex_deep).reshape(len(seeds),-1)\n",
    "nonconvexr=np.array(nonconvex).reshape(len(seeds),-1)\n",
    "linearr=np.array(linear).reshape(len(seeds),-1)\n",
    "\n",
    "ns=len(nvec)\n",
    "nseed=len(seeds)\n",
    "convexm=np.mean(convexr,axis=0)\n",
    "convex_deepm=np.mean(convex_deepr,axis=0)\n",
    "nonconvexm=np.mean(nonconvexr,axis=0)\n",
    "linearm=np.mean(linearr,axis=0)\n",
    "\n",
    "convexs=np.std(convexr,axis=0)\n",
    "convex_deeps=np.std(convex_deepr,axis=0)\n",
    "nonconvexs=np.std(nonconvexr,axis=0)\n",
    "linears=np.std(linearr,axis=0)\n",
    "\n",
    "\n",
    "colors=[\"blue\", \"orange\", \"green\", \"red\"]\n",
    "\n",
    "\n",
    "plt.plot(nvec, convexm, color=colors[0], label=\"convex\")\n",
    "upper=convexm+convexs\n",
    "lower=convexm-convexs\n",
    "plt.fill_between(nvec, lower, upper, color=colors[0], alpha=.1)\n",
    "plt.plot(nvec, convex_deepm, color=colors[1], label=\"convex_deep\")\n",
    "upper=convex_deepm+convex_deeps\n",
    "lower=convex_deepm-convex_deeps\n",
    "plt.fill_between(nvec, lower, upper, color=colors[1], alpha=.1)\n",
    "\n",
    "plt.plot(nvec, nonconvexm, color=colors[2], label=\"nonconvex\")\n",
    "upper=nonconvexm+nonconvexs\n",
    "lower=nonconvexm-nonconvexs\n",
    "plt.fill_between(nvec, lower, upper, color=colors[2], alpha=.1)\n",
    "    \n",
    "plt.plot(nvec, linearm, color=colors[3], label=\"linear\")\n",
    "upper=linearm+linears\n",
    "lower=linearm-linears\n",
    "plt.fill_between(nvec, lower, upper, color=colors[3], alpha=.1)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "import os.path\n",
    "save=False\n",
    "if save:\n",
    "    figpath=\"Finetuning_cvx_numneurons\"+str(num_neurons)+\"_depth\"+str(depth)+\"_nvec\"+str(len(nvec))+\"_numseed\"+str(len(seeds))+\"_beta\"+str(beta)+\".pdf\"\n",
    "    if os.path.isfile(figpath):\n",
    "        figpath=\"Finetuning_cvx_numneurons\"+str(num_neurons)+\"_depth\"+str(depth)+\"_nvec\"+str(len(nvec))+\"_numseed\"+str(len(seeds))+\"_beta\"+str(beta)+\"v2.pdf\"\n",
    "\n",
    "    plt.savefig(figpath, format='pdf', dpi=300) \n",
    "    filepath=\"Finetuning_cvx_numneurons\"+str(num_neurons)+\"_depth\"+str(depth)+\"_nvec\"+str(len(nvec))+\"_numseed\"+str(len(seeds))+\"_beta\"+str(beta)+\".npz\"\n",
    "    if os.path.isfile(filepath):\n",
    "        filepath=\"Finetuning_cvx_numneurons\"+str(num_neurons)+\"_depth\"+str(depth)+\"_nvec\"+str(len(nvec))+\"_numseed\"+str(len(seeds))+\"_beta\"+str(beta)+\"v2.npz\"\n",
    "\n",
    "    np.savez(filepath, nvec=nvec, convex=convexr, convex_deep=convex_deepr, nonconvex=nonconvexr, linear=linearr, seeds=seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convexm)\n",
    "print(convex_deepm)\n",
    "print(nonconvexm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convexm)\n",
    "print(convex_deepm)\n",
    "print(nonconvexm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonconvexr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"Finetuning_cvx_numneurons\"+str(num_neurons)+\"_nvec\"+str(len(nvec))+\"_numseed\"+str(len(seeds))+\"_beta\"+str(beta)+\".npz\"\n",
    "\n",
    "saveddata=np.load(filepath)\n",
    "convexr=saveddata[\"convex\"]\n",
    "convex_deepr=saveddata[\"convex_deep\"]\n",
    "nonconvexr=saveddata[\"nonconvex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonconvexr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v=np.linalg.svd(embeds)\n",
    "plt.semilogy(s.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
